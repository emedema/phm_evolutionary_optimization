{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set Global Variables\n",
    "\"\"\"\n",
    "# place holder bit string until we get passed the true one\n",
    "BITSTRING = '0100101010000011100101001101'\n",
    "\n",
    "# specific GA hyperparameters\n",
    "BERT_LAYER = int(BITSTRING[0:4]) #done\n",
    "FINE_TUNING_ARCHITECTURE = 0 #int(BITSTRING[4:6]) #td\n",
    "CNN_DROPOUT = 0.2 #int(BITSTRING[6:10]) #td\n",
    "CNN_KERNEL_SIZE = 2 #int(BITSTRING[10:13]) #td\n",
    "BILSTM_DROPOUT = int(BITSTRING[13:17]) #td\n",
    "BILSTM_OUTPUT_NEURON = int(BITSTRING[17:27]) #td\n",
    "FINAL_ACTIVATION_NEURON = int(BITSTRING[27]) #done\n",
    "\n",
    "if BERT_LAYER > 13:\n",
    "    BERT_LAYER = 13 ## embedding layer = 0 and then layers 1 - 12, make sure these numbers get translated to the right values\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1 #10\n",
    "NUM_FEATURES = 4\n",
    "## length of tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Dataset - may remove if we do this offline\n",
    "\"\"\" \n",
    "## load in the dataset\n",
    "col1_names=['id', 'tweet_id', 'text', 'username']\n",
    "col2_names = ['tweet_id', 'disease', 'label']\n",
    "df1 = pd.read_csv(\"phm2017_tweets.csv\", names=col1_names, header=None)\n",
    "df2 = pd.read_csv(\"PHM2017.csv\", names=col2_names, header=None)\n",
    "df = pd.merge(df1, df2, on=\"tweet_id\")\n",
    "df = df.drop(['id', 'tweet_id', 'username', 'disease'], axis=1)\n",
    "df['label'] = df['label'].astype(int)\n",
    "print(df.dtypes)\n",
    "\n",
    "## get shape\n",
    "shape = df.shape\n",
    "print(shape)\n",
    "\n",
    "# check class distribution\n",
    "print(df['label'].value_counts(normalize = True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make train-test splits\n",
    "\"\"\"\n",
    "# split into train test split\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'],\n",
    "                                                                    random_state=2018,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                random_state=2018,\n",
    "                                                                test_size=0.5,\n",
    "                                                                stratify=temp_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize \n",
    "\"\"\"\n",
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "max_seq_len = 25 ## change this!\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding= 'longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding= 'longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length=max_seq_len,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Tensors\n",
    "\"\"\"\n",
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Dataloaders\n",
    "\"\"\"\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Model\n",
    "\"\"\"\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # one layer CNN\n",
    "        if FINE_TUNING_ARCHITECTURE == 0:\n",
    "            # may need to reshape\n",
    "            self.conv = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                              padding='valid', stride=1)\n",
    "            self.maxpool = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "            self.dropout = nn.Dropout(CNN_DROPOUT)\n",
    "\n",
    "        # two layer CNN\n",
    "        elif FINE_TUNING_ARCHITECTURE == 1:\n",
    "            self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                              padding='valid', stride=1)\n",
    "            self.maxpool1 = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "            self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                              padding='valid', stride=1)\n",
    "            self.maxpool2 = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "            self.dropout2 = nn.Dropout(CNN_DROPOUT)\n",
    "\n",
    "        elif FINE_TUNING_ARCHITECTURE == 2:\n",
    "            self.lstm = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "            self.dropout = nn.Dropout(BILSTM_DROPOUT)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "            self.dropout = nn.Dropout(BILSTM_DROPOUT)\n",
    "\n",
    "        # dropout layer\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        #self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        #self.fc1 = nn.Linear(768, 512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        #self.fc2 = nn.Linear(512, 4)  # changed from 2\n",
    "\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        # softmax activation function\n",
    "        if FINAL_ACTIVATION_NEURON == 0:\n",
    "            self.softmax = nn.LogSoftmax(dim=1) #is dim = 1 right here?\n",
    "        else:\n",
    "            self.sigmoid = nn.Sigmoid() #is dim = 1 right here?\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        # pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "\n",
    "        #x = self.fc1(cls_hs)\n",
    "\n",
    "        #x = self.relu(x)\n",
    "\n",
    "        #x = self.dropout(x)\n",
    "        # one layer CNN\n",
    "        if FINE_TUNING_ARCHITECTURE == 0:\n",
    "            # may need to reshape\n",
    "            x = self.conv(cls_hs)\n",
    "            x = self.maxpool(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # two layer CNN\n",
    "        elif FINE_TUNING_ARCHITECTURE == 1:\n",
    "            x = self.conv1(x)\n",
    "            x = self.maxpool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.maxpool2(x)\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        elif FINE_TUNING_ARCHITECTURE == 2:\n",
    "            self.lstm = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "            self.dropout(BILSTM_DROPOUT)\n",
    "\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "            self.dropout(BILSTM_DROPOUT)\n",
    "\n",
    "        # output layer\n",
    "        #x = self.fc2(x)\n",
    "\n",
    "        # apply final activation\n",
    "        if FINAL_ACTIVATION_NEURON == 0:\n",
    "            x = self.softmax(x)\n",
    "        else:\n",
    "            x = self.sigmoid(x)\n",
    "\n",
    "        #x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "\n",
    "# compute the class weights\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes = np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(class_wts)\n",
    "\n",
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Training & Evaluation\n",
    "\"\"\"\n",
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train & Output F1-Score\n",
    "\"\"\"\n",
    "\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, EPOCHS))\n",
    "\n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    # evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "f1 = f1_score(test_y, preds, average='weighted')\n",
    "#print(classification_report(test_y, preds))\n",
    "print(f1)\n",
    "# confusion matrix\n",
    "#pd.crosstab(test_y, preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}