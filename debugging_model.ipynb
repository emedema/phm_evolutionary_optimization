{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "TO DO \n",
    "- remove tweet dataset preprocessing stuff and use the preprocessed dataset\n",
    "- turn into a function that takes a bitstring as the only parameter and only returns the f1score \n",
    "    - maybe just call it fitness \n",
    "\"\"\"\n",
    "\n",
    "def fitness(bitstring):\n",
    "\n",
    "    ### GLOBAL VARIABLES ###\n",
    "\n",
    "    # specific GA hyperparameters\n",
    "    BERT_LAYER = int(bitstring[0:4]) #done\n",
    "    FINE_TUNING_ARCHITECTURE = 0 #int(bitstring[4:6]) #td\n",
    "    CNN_DROPOUT = 0.2 #int(bitstring[6:10]) #done\n",
    "    CNN_KERNEL_SIZE = 2 #int(bitstring[10:13]) #done\n",
    "    BILSTM_DROPOUT = int(bitstring[13:17]) #td\n",
    "    BILSTM_OUTPUT_NEURON = int(bitstring[17:27]) #td\n",
    "    FINAL_ACTIVATION_NEURON = int(bitstring[27]) #done\n",
    "\n",
    "    if BERT_LAYER > 13:\n",
    "        BERT_LAYER = 13 ## embedding layer = 0 and then layers 1 - 12, make sure these numbers get translated to the right values\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 1 #10\n",
    "    NUM_FEATURES = 4\n",
    "    ## need length of tweet\n",
    "\n",
    "    ## load in the dataset\n",
    "    col1_names=['id', 'tweet_id', 'text', 'username']\n",
    "    col2_names = ['tweet_id', 'disease', 'label']\n",
    "    df1 = pd.read_csv(\"phm2017_tweets.csv\", names=col1_names, header=None)\n",
    "    df2 = pd.read_csv(\"PHM2017.csv\", names=col2_names, header=None)\n",
    "    df = pd.merge(df1, df2, on=\"tweet_id\")\n",
    "    df = df.drop(['id', 'tweet_id', 'username', 'disease'], axis=1)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    ## get shape\n",
    "    shape = df.shape\n",
    "\n",
    "    # uncomment if you want to see what the df looks liek\n",
    "    #print(shape)\n",
    "\n",
    "\n",
    "    # uncomment if you want to check class distribution\n",
    "    #print(df['label'].value_counts(normalize = True))\n",
    "\n",
    "\n",
    "    # split into train test split\n",
    "    train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'],\n",
    "                                                                        random_state=2018,\n",
    "                                                                        test_size=0.3,\n",
    "                                                                        stratify=df['label'])\n",
    "\n",
    "    # we will use temp_text and temp_labels to create validation and test set\n",
    "    val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                    random_state=2018,\n",
    "                                                                    test_size=0.5,\n",
    "                                                                    stratify=temp_labels)\n",
    "\n",
    "    # import BERT-base pretrained model\n",
    "    bert = AutoModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "\n",
    "    # Load the BERT tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    ## uncomment if you just want to see an example\n",
    "    # sample data\n",
    "    #text = [\"this is a bert model tutorial\", \"we will fine-tune a bert model\"]\n",
    "\n",
    "    # encode text\n",
    "    #sent_id = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)\n",
    "\n",
    "    # output\n",
    "    #print(sent_id)\n",
    "\n",
    "    ############### TOKENIZATION ###############\n",
    "    # get length of all the messages in the train set\n",
    "    seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "    pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "    max_seq_len = 280 ## change this maybe?\n",
    "\n",
    "    # tokenize and encode sequences in the training set\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        train_text.tolist(),\n",
    "        max_length=max_seq_len,\n",
    "        padding= 'longest',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # tokenize and encode sequences in the validation set\n",
    "    tokens_val = tokenizer.batch_encode_plus(\n",
    "        val_text.tolist(),\n",
    "        max_length=max_seq_len,\n",
    "        padding= 'longest',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    # tokenize and encode sequences in the test set\n",
    "    tokens_test = tokenizer.batch_encode_plus(\n",
    "        test_text.tolist(),\n",
    "        max_length=max_seq_len,\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "\n",
    "    ############### CREATE TENSORS ###############\n",
    "\n",
    "    # for train set\n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "    train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "    # for validation set\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "    val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "    # for test set\n",
    "    test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "    test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "    test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "    ############### CREATE DATALOOADERS ###############\n",
    "\n",
    "    # wrap tensors\n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "    # sampler for sampling the data during training\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "\n",
    "    # dataLoader for train set\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # wrap tensors\n",
    "    val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "    # sampler for sampling the data during training\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "    # dataLoader for validation set\n",
    "    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # freeze only the specific layers we want\n",
    "    modules = [bert.embeddings, *bert.encoder.layer[:BERT_LAYER]]\n",
    "    for module in modules:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    class BERT_Arch(nn.Module):\n",
    "\n",
    "        def __init__(self, bert):\n",
    "            super(BERT_Arch, self).__init__()\n",
    "\n",
    "            self.bert = bert\n",
    "\n",
    "            # one layer CNN\n",
    "            if FINE_TUNING_ARCHITECTURE == 0:\n",
    "                self.dropout1 = nn.Dropout(CNN_DROPOUT)\n",
    "                self.conv = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                                  padding='valid')\n",
    "                self.relu = nn.ReLU()\n",
    "                self.maxpool = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "                self.fc = nn.Linear(15, 4)\n",
    "\n",
    "            # two layer CNN\n",
    "            elif FINE_TUNING_ARCHITECTURE == 1:\n",
    "                self.dropout2 = nn.Dropout(CNN_DROPOUT)\n",
    "                self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                                  padding='valid')\n",
    "                self.relu1 = nn.ReLU()\n",
    "                self.maxpool1 = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "                self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=CNN_KERNEL_SIZE,\n",
    "                                                  padding='valid')\n",
    "                self.relu2 = nn.ReLU()\n",
    "                self.maxpool2 = nn.MaxPool1d(kernel_size=CNN_KERNEL_SIZE)\n",
    "                self.fc1 = nn.Linear(15,4)\n",
    "\n",
    "\n",
    "            # one layer BiLSTM\n",
    "            elif FINE_TUNING_ARCHITECTURE == 2:\n",
    "                self.lstm1 = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "                self.dropout3 = nn.Dropout(BILSTM_DROPOUT)\n",
    "\n",
    "            # two layer BiLSTM\n",
    "            else:\n",
    "                self.lstm2 = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "                self.lstm3 = nn.LSTM(768, seq_len, NUM_FEATURES, batch_first=True, bidirectional=True)\n",
    "                self.dropout4 = nn.Dropout(BILSTM_DROPOUT)\n",
    "\n",
    "\n",
    "            # softmax activation function\n",
    "            if FINAL_ACTIVATION_NEURON == 0:\n",
    "                self.softmax = nn.LogSoftmax(dim=1) #is dim = 1 right here?\n",
    "            else:\n",
    "                self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # define the forward pass\n",
    "        def forward(self, sent_id, mask):\n",
    "            # pass the inputs to the model\n",
    "            _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "\n",
    "            # one layer CNN\n",
    "            if FINE_TUNING_ARCHITECTURE == 0:\n",
    "                cls_hs = cls_hs.transpose(1, 0)\n",
    "                x = self.dropout1(cls_hs)\n",
    "                x = self.conv(x)\n",
    "                x = self.relu(x)\n",
    "                x = self.maxpool(x)\n",
    "                x = self.fc(x)\n",
    "\n",
    "            # two layer CNN\n",
    "            elif FINE_TUNING_ARCHITECTURE == 1:\n",
    "                cls_hs = cls_hs.transpose(1, 0)\n",
    "                x = self.dropout2(cls_hs)\n",
    "                x = self.conv1(x)\n",
    "                x = self.relu1(x)\n",
    "                x = self.maxpool1(x)\n",
    "                x = self.conv2(x)\n",
    "                x = self.relu2(x)\n",
    "                x = self.maxpool2(x)\n",
    "                x = self.fc1(x)\n",
    "\n",
    "            # one layer BiLSTM\n",
    "            elif FINE_TUNING_ARCHITECTURE == 2:\n",
    "                x = self.lstm1(cls_hs)\n",
    "                x = self.dropout3(x)\n",
    "\n",
    "            # two layer BiLSTM\n",
    "            else:\n",
    "                x = self.lstm2(cls_hs)\n",
    "                x = self.dropout4(x)\n",
    "\n",
    "            # apply final activation\n",
    "            if FINAL_ACTIVATION_NEURON == 0:\n",
    "                x = self.softmax(x)\n",
    "                print(x.shape)\n",
    "            else:\n",
    "                x = self.sigmoid(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "    # pass the pre-trained BERT to our define architecture\n",
    "    model = BERT_Arch(bert)\n",
    "\n",
    "    # push the model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # define the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "\n",
    "    # compute the class weights\n",
    "    class_wts = compute_class_weight(class_weight='balanced', classes = np.unique(train_labels), y=train_labels)\n",
    "\n",
    "    # uncomment if you want to print classweights\n",
    "    #print(class_wts)\n",
    "\n",
    "    # convert class weights to tensor\n",
    "    weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "    weights = weights.to(device)\n",
    "\n",
    "    # loss function\n",
    "    cross_entropy  = nn.NLLLoss(weight=weights)\n",
    "\n",
    "    # number of training epochs\n",
    "\n",
    "\n",
    "    # function to train the model\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "\n",
    "        # empty list to save model predictions\n",
    "        total_preds = []\n",
    "\n",
    "        # iterate over batches\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # progress update after every 50 batches.\n",
    "            if step % 50 == 0 and not step == 0:\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "            # push the batch to gpu\n",
    "            batch = [r.to(device) for r in batch]\n",
    "\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            #print(batch[0].shape)\n",
    "\n",
    "            # clear previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # get model predictions for the current batch\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            # add on to the total loss\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            # backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # model predictions are stored on GPU. So, push it to CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # append the model predictions\n",
    "            total_preds.append(preds)\n",
    "\n",
    "        # compute the training loss of the epoch\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "        # reshape the predictions in form of (number of samples, no. of classes)\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "        # returns the loss and predictions\n",
    "        return avg_loss, total_preds\n",
    "\n",
    "\n",
    "    # function for evaluating the model\n",
    "    def evaluate():\n",
    "        print(\"\\nEvaluating...\")\n",
    "\n",
    "        # deactivate dropout layers\n",
    "        model.eval()\n",
    "\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "\n",
    "        # empty list to save the model predictions\n",
    "        total_preds = []\n",
    "\n",
    "        # iterate over batches\n",
    "        for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "            # Progress update every 50 batches.\n",
    "            if step % 50 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                #elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "            # push the batch to gpu\n",
    "            batch = [t.to(device) for t in batch]\n",
    "\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # deactivate autograd\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # model predictions\n",
    "                preds = model(sent_id, mask)\n",
    "\n",
    "                # compute the validation loss between actual and predicted values\n",
    "                loss = cross_entropy(preds, labels)\n",
    "\n",
    "                total_loss = total_loss + loss.item()\n",
    "\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "\n",
    "                total_preds.append(preds)\n",
    "\n",
    "        # compute the validation loss of the epoch\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "        # reshape the predictions in form of (number of samples, no. of classes)\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "        return avg_loss, total_preds\n",
    "\n",
    "\n",
    "    # set initial loss to infinite\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    # empty lists to store training and validation loss of each epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    # for each epoch\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print('\\n Epoch {:} / {:}'.format(epoch + 1, EPOCHS))\n",
    "\n",
    "        # train model\n",
    "        train_loss, _ = train()\n",
    "\n",
    "        # evaluate model\n",
    "        valid_loss, _ = evaluate()\n",
    "\n",
    "        # save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "    #load weights of best model\n",
    "    path = 'saved_weights.pt'\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    # get predictions for test data\n",
    "    with torch.no_grad():\n",
    "        preds = model(test_seq.to(device), test_mask.to(device))\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    # model's performance\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    f1 = f1_score(test_y, preds, average='weighted')\n",
    "\n",
    "    # confusion matrix\n",
    "    #pd.crosstab(test_y, preds)\n",
    "\n",
    "    return f1\n",
    "\n",
    "bitstring = '0100101010000011100101001101'\n",
    "fitness(bitstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f497d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
